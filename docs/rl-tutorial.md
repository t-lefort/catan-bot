# Reinforcement Learning pour CatanBot : Guide pour d√©butants

Ce document explique les concepts de Reinforcement Learning (RL) appliqu√©s √† CatanBot, de fa√ßon accessible pour quelqu'un qui n'a jamais fait de RL.

---

## Table des mati√®res

1. [L'id√©e de base : apprendre par l'exp√©rience](#1-lid√©e-de-base--apprendre-par-lexp√©rience)
2. [Les 3 composantes cl√©s du RL](#2-les-3-composantes-cl√©s-du-rl)
3. [Architectures de r√©seaux en RL](#3-architectures-de-r√©seaux-en-rl)
4. [Comment d√©finir la structure du r√©seau ?](#4-comment-d√©finir-la-structure-du-r√©seau-)
5. [MLP vs CNN vs GNN](#5-mlp-vs-cnn-vs-gnn)
6. [Peut-on battre des humains experts ?](#6-peut-on-battre-des-humains-experts-)
7. [Nos choix pour CatanBot](#7-nos-choix-pour-catanbot)

---

## 1. L'id√©e de base : apprendre par l'exp√©rience

### Analogie : enseigner les √©checs √† un enfant

Imagine que tu apprends √† un enfant √† jouer aux √©checs. Tu ne lui donnes pas un livre de 500 pages de strat√©gies. √Ä la place :

1. **Il joue** une partie (il fait des coups, m√™me mauvais)
2. **Il voit le r√©sultat** (il gagne ou perd)
3. **Il ajuste** : "Ah, sacrifier ma reine, c'√©tait pas malin !"
4. **Il rejoue** et devient progressivement meilleur

### Le Reinforcement Learning fait exactement √ßa

Le **Reinforcement Learning (RL)** applique ce principe √† un ordinateur. L'agent apprend en **jouant**, pas en lisant des r√®gles √©crites par des humains.

**Diff√©rence avec d'autres approches** :
- **Apprentissage supervis√©** : On donne des exemples "bonne action" / "mauvaise action"
- **Reinforcement Learning** : L'agent d√©couvre seul quelles actions sont bonnes, en jouant des milliers de parties

---

## 2. Les 3 composantes cl√©s du RL

### 2.1 L'Observation (ce que l'agent "voit")

Quand tu joues √† Catan, tu observes :
- Le plateau (hexagones, ports, position du voleur)
- Tes ressources (brique, bl√©, minerai, bois, laine)
- Les ressources de l'adversaire (visibles)
- Les colonies/villes/routes plac√©es
- Les cartes de d√©veloppement
- Qui d√©tient le chemin le plus long, la plus grande arm√©e
- Le score actuel

**Pour l'ordinateur**, toutes ces informations sont converties en chiffres ! C'est l'`ObservationTensor` (d√©j√† impl√©ment√© dans `catan/rl/features.py`) :

```python
# Exemple simplifi√©
observation = {
    "board": [0, 1, 0, ...],           # Ressources des hexagones
    "hands": [[2, 3, 1, 0, 4],         # Mes ressources (ego-centr√©)
              [1, 0, 2, 1, 3]],        # Ressources adversaire
    "settlements": [1, 0, 0, ...],     # Positions de mes colonies
    "metadata": [0, 0, 1, 0.15, ...],  # Phase, tour, titres
    # ... etc
}
```

**Point cl√© : Normalisation**
Tous ces chiffres sont **normalis√©s** (g√©n√©ralement entre 0 et 1) pour faciliter l'apprentissage. Par exemple :
- Ressources : divis√©es par 19 (max th√©orique)
- Tour : divis√© par 200 (normalisation temporelle)
- VP : divis√©s par 15 (objectif de victoire)

**Point cl√© : Perspective ego-centr√©e**
Les informations du joueur actuel sont toujours √† l'index 0, l'adversaire √† l'index 1. Ainsi l'agent apprend une seule strat√©gie, pas deux (joueur 0 vs joueur 1).

---

### 2.2 L'Action (ce que l'agent d√©cide)

√Ä ton tour √† Catan, tu peux faire plusieurs choses :
- Lancer les d√©s
- Construire une route, une colonie, une ville
- √âchanger avec la banque ou l'adversaire
- Jouer une carte d√©veloppement (chevalier, progr√®s)
- Terminer ton tour

**L'ordinateur** a une liste de toutes les actions possibles (c'est l'`ActionEncoder` dans `catan/rl/actions.py`). Le catalogue contient **toutes** les actions th√©oriques :
- PlaceRoad(edge_id=0), PlaceRoad(edge_id=1), ..., PlaceRoad(edge_id=71)
- PlaceSettlement(vertex_id=0), ..., PlaceSettlement(vertex_id=53)
- TradeBank(give={BRICK:4}, receive={ORE:1}), ...
- Etc.

Mais √† un instant donn√©, **la plupart de ces actions sont ill√©gales** (ex: construire une ville si tu n'as pas 2 minerai + 3 bl√©).

**Masque d'actions l√©gales**
On cr√©e donc un **masque bool√©en** :
```python
legal_actions_mask = [1, 0, 1, 1, 0, 0, 1, ...]
# 1 = action l√©gale, 0 = action interdite
```

Ainsi, l'agent ne choisira **jamais** une action interdite (voir section 2.3).

---

### 2.3 Le R√©seau de neurones (le "cerveau" de l'agent)

C'est le programme PyTorch qui :

1. **Prend en entr√©e** : l'observation (plateau, ressources, etc.)
2. **Calcule** avec des couches de neurones
3. **Renvoie 2 sorties** :
   - **Politique** (policy) : "Quelle action choisir ?"
   - **Valeur** (value) : "Est-ce que je suis en train de gagner ?"

#### Anatomie du r√©seau : les 2 t√™tes

Imagine un r√©seau en forme de **Y** :

```
                [ObservationTensor]
                         |
                   [Flatten tout]
                         |
                  [Encoder commun]
               (couches fully-connected)
                  512 ‚Üí 256 ‚Üí 128
                         |
                +--------+--------+
                |                 |
         [T√™te Politique]    [T√™te Valeur]
          128 ‚Üí action_size   128 ‚Üí 1
                |                 |
         "Quelle action?"    "Suis-je gagnant?"
          (logits + mask)      (valeur ‚àà [-1,1])
```

---

#### T√™te 1 : La Politique (Policy Head)

**Objectif** : D√©cider quelle action jouer.

**Fonctionnement** :
1. Le r√©seau calcule un **score brut** (appel√© "logit") pour chaque action possible
2. On **applique le masque** : les actions ill√©gales re√ßoivent un score de `-‚àû`
3. On transforme les scores en **probabilit√©s** via softmax :
   ```python
   logits = [2.3, -1.5, 0.8, -0.2, ...]  # Scores bruts
   mask   = [1,    0,    1,    1,   ...]  # 0 = ill√©gal

   # On masque les actions ill√©gales
   logits_masqu√©s = [2.3, -inf, 0.8, -0.2, ...]

   # Softmax ‚Üí probabilit√©s
   probs = softmax(logits_masqu√©s)  # [0.62, 0.0, 0.14, 0.09, ...]
   ```

**R√©sultat** : L'agent obtient une distribution de probabilit√© sur les actions l√©gales :
- Action A : 62% de chance
- Action B : 0% (ill√©gale, masqu√©e)
- Action C : 14% de chance
- Action D : 9% de chance
- ...

**Pourquoi des probabilit√©s et pas juste "la meilleure" ?**
- Au d√©but de l'entra√Ænement, l'agent est nul ‚Üí il doit **explorer** (essayer des actions vari√©es)
- Avec l'exp√©rience, il va concentrer ses probabilit√©s sur les bonnes actions (exploitation)
- Ce compromis exploration/exploitation est au c≈ìur du RL

---

#### T√™te 2 : La Valeur (Value Head)

**Objectif** : Estimer si la situation actuelle est favorable ou d√©favorable.

**Fonctionnement** :
Le r√©seau renvoie un seul chiffre entre **-1** et **+1** :
- `+1` = "Je vais presque certainement gagner !"
- `0` = "C'est 50/50, la partie est serr√©e..."
- `-1` = "Je suis tr√®s mal parti, je vais perdre..."

**Pourquoi c'est utile ?**
- Pendant l'entra√Ænement, on compare cette pr√©diction avec le **r√©sultat r√©el** de la partie :
  - Si l'agent pensait gagner (+0.8) mais a perdu ‚Üí il ajuste ses poids pour √™tre plus pessimiste dans cette situation
  - Si l'agent pensait perdre (-0.6) mais a gagn√© ‚Üí il ajuste pour √™tre plus optimiste
- La t√™te valeur permet aussi de **stabiliser l'entra√Ænement** en r√©duisant la variance des gradients (concept avanc√©)

---

#### L'Encoder : la partie commune

Avant de s√©parer en 2 t√™tes, on doit **comprendre** l'observation brute. C'est le r√¥le de l'**encoder** :

```
[ObservationTensor brut]
  board       : (19 hexagones, 6 features)  ‚Üí 114 floats
  roads       : (72 edges)                   ‚Üí 72 floats
  settlements : (54 vertices)                ‚Üí 54 floats
  hands       : (2 joueurs, 5 ressources)    ‚Üí 10 floats
  dev_cards   : (2 joueurs, 5 types)         ‚Üí 10 floats
  bank        : (5 ressources)               ‚Üí 5 floats
  metadata    : (phase, tour, titres, etc.)  ‚Üí 10 floats

                    ‚Üì [Flatten tout]

         [Vecteur de ~275 floats]

                    ‚Üì [Couche FC : 275 ‚Üí 512 neurones]
                    ‚Üì [ReLU + LayerNorm]
                    ‚Üì [Couche FC : 512 ‚Üí 256 neurones]
                    ‚Üì [ReLU + LayerNorm]
                    ‚Üì [Couche FC : 256 ‚Üí 128 neurones]

         [Repr√©sentation compacte : 128 features]
                 (encodage latent)
```

Ces 128 features capturent l'essence de la situation (ex: "je suis en avance", "je manque de minerai", "l'adversaire a beaucoup de routes").

Ensuite, ces 128 features sont envoy√©es **simultan√©ment** aux 2 t√™tes (politique et valeur).

---

#### Le masquage d'actions : CRUCIAL !

Imagine que le r√©seau calcule des scores et dit : "Je veux construire une ville !"... mais tu n'as que 1 minerai (il en faut 3).

**Sans masque** :
- L'agent essaie l'action BuildCity()
- Le moteur rejette l'action (erreur)
- Crash ou comportement incoh√©rent

**Avec masque** :
```python
logits = [2.3, -1.5, 0.8, ...]  # Scores bruts (BuildRoad, BuildCity, TradeBank, ...)
mask   = [1,    0,    1,   ...]  # BuildCity est ill√©gal (mask=0)

# On met -inf sur les actions ill√©gales
masked_logits = torch.where(mask, logits, torch.tensor(-float('inf')))
# ‚Üí [2.3, -inf, 0.8, ...]

# Softmax ‚Üí les actions ill√©gales ont 0% de probabilit√©
probs = F.softmax(masked_logits, dim=-1)
# ‚Üí [0.85, 0.0, 0.15, ...]  BuildCity a maintenant 0% de chance
```

Ainsi, l'agent ne choisira **jamais** une action interdite ! C'est une contrainte **hard** (pas apprise, mais impos√©e).

---

## 3. Architectures de r√©seaux en RL

**Question** : Est-ce toujours 2 t√™tes (politique + valeur) en RL ?

**R√©ponse** : Non ! Il existe plusieurs architectures selon le contexte. Voici les principales :

---

### 3.1 R√©seau √† 2 t√™tes partag√©es (Actor-Critic, ce qu'on utilise)

```
Observation ‚Üí Encoder ‚Üí ‚î¨‚Üí T√™te Politique (Actor)
                        ‚îî‚Üí T√™te Valeur (Critic)
```

**Utilis√© par** :
- AlphaZero (√©checs, Go, shogi)
- PPO (Proximal Policy Optimization)
- A2C (Advantage Actor-Critic)

**Avantages** :
- L'encoder est **partag√©** ‚Üí √©conomie de calcul et de m√©moire
- Apprentissage plus stable : la valeur guide la politique
- Fonctionne bien pour les jeux de plateau (actions discr√®tes)

**Inconv√©nient** :
- Si les objectifs politique/valeur sont trop diff√©rents, ils peuvent interf√©rer ("tirer dans des directions oppos√©es")

---

### 3.2 R√©seaux s√©par√©s (Actor-Critic avec 2 r√©seaux ind√©pendants)

```
Observation ‚Üí R√©seau Politique (Actor)   ‚Üí logits
Observation ‚Üí R√©seau Valeur (Critic)     ‚Üí valeur
```

**Utilis√© par** :
- DDPG (Deep Deterministic Policy Gradient)
- TD3 (Twin Delayed DDPG)
- SAC (Soft Actor-Critic)

**Avantages** :
- Chaque r√©seau est **ind√©pendant** ‚Üí pas d'interf√©rence
- Plus flexible pour ajuster les taux d'apprentissage s√©par√©ment

**Inconv√©nient** :
- 2√ó plus de param√®tres ‚Üí plus lent, plus gourmand en m√©moire
- N√©cessite plus de donn√©es pour converger

**Quand l'utiliser** : Actions continues (ex: angle de tir, vitesse d'un robot)

---

### 3.3 R√©seau de Q-valeurs (DQN)

```
Observation ‚Üí R√©seau ‚Üí Q(s, a‚ÇÄ), Q(s, a‚ÇÅ), Q(s, a‚ÇÇ), ...
```

**Utilis√© par** :
- DQN (Deep Q-Network, Atari)
- Double DQN, Dueling DQN

**Principe** :
Le r√©seau estime directement la **valeur** de chaque action (Q-value).
L'agent choisit l'action avec la plus grande Q-value (greedy).

**Avantages** :
- Plus simple conceptuellement (1 seule sortie)
- Fonctionne bien avec un replay buffer

**Inconv√©nients** :
- Ne g√®re pas bien les **actions continues**
- N√©cessite beaucoup de m√©moire (replay buffer)
- Moins efficace pour les espaces d'actions tr√®s grands (>10k actions)

**Quand l'utiliser** : Jeux vid√©o avec actions discr√®tes (Atari, etc.)

---

### 3.4 Transformers (moderne)

```
[Observation t-n, ..., Observation t] ‚Üí Transformer ‚Üí Politique + Valeur
```

**Utilis√© par** :
- Decision Transformer (2021)
- Gato (DeepMind, 2022)
- Transdreamer

**Principe** :
Traite l'historique des √©tats comme une **s√©quence** (comme GPT traite du texte).
Peut apprendre des d√©pendances temporelles complexes.

**Avantages** :
- Capture la m√©moire √† long terme (ex: "il y a 20 tours, j'ai vu qu'il avait beaucoup de minerai")
- Peut faire du few-shot learning (apprendre avec peu d'exemples)

**Inconv√©nients** :
- **Tr√®s gourmand** en calcul (attention mechanism est O(n¬≤))
- N√©cessite √©norm√©ment de donn√©es
- Complexe √† impl√©menter et d√©boguer

**Quand l'utiliser** : Environnements partiellement observables, ou quand l'historique est crucial

---

### 3.5 Tableau r√©capitulatif

| Architecture | Avantages | Inconv√©nients | Cas d'usage |
|--------------|-----------|---------------|-------------|
| **2 t√™tes partag√©es** | Efficace, stable, √©conome | Possible interf√©rence | **Jeux de plateau, actions discr√®tes** |
| **2 r√©seaux s√©par√©s** | Flexible, pas d'interf√©rence | 2√ó param√®tres, plus lent | Actions continues (robotique) |
| **DQN** | Simple, replay buffer | Mal adapt√© aux actions continues | Jeux vid√©o (Atari) |
| **Transformer** | M√©moire √† long terme | Tr√®s co√ªteux, beaucoup de donn√©es | Environnements partiellement observables |

---

### Pourquoi on a choisi 2 t√™tes partag√©es pour CatanBot ?

‚úÖ **Efficace** : L'encoder partage les calculs entre politique et valeur
‚úÖ **Prouv√©** : AlphaZero (√©checs, Go) et PPO utilisent cette architecture avec succ√®s
‚úÖ **Adapt√© √† Catan** : Espace d'actions discret (construire une route √† la position X)
‚úÖ **Entra√Ænement stable** : La t√™te valeur aide √† r√©duire la variance des gradients
‚úÖ **Pas d'overkill** : Pas besoin de transformers pour un jeu √† information parfaite

---

## 4. Comment d√©finir la structure du r√©seau ?

**Question** : Comment sait-on qu'il faut 512 ‚Üí 256 ‚Üí 128 neurones ? Pourquoi pas 1000 ‚Üí 500 ‚Üí 100 ?

**R√©ponse** : **C'est un art autant qu'une science !** Il n'y a **pas de formule magique**, mais des r√®gles empiriques et beaucoup d'exp√©rimentation.

---

### 4.1 R√®gles de base pour la taille de l'encoder

#### Trop petit ‚Üí Underfitting
- Le r√©seau n'a pas assez de **capacit√©** pour apprendre des strat√©gies complexes
- Sympt√¥mes : La loss descend un peu puis plafonne rapidement, l'agent reste faible

#### Trop gros ‚Üí Overfitting
- Le r√©seau m√©morise des situations sp√©cifiques au lieu de g√©n√©raliser
- Sympt√¥mes : Entra√Ænement lent, performances variables, surapprentissage sur certains adversaires

#### Sweet spot
- Le r√©seau est **juste assez grand** pour capturer les patterns importants
- On peut toujours augmenter apr√®s si √ßa ne suffit pas

---

### 4.2 R√®gle empirique pour Catan

Pour dimensionner un encoder, on regarde :
- **Taille de l'observation** : ~275 features (plateau + ressources + metadata)
- **Taille de l'espace d'actions** : ~200-500 actions (selon le catalogue)
- **Complexit√© du jeu** : Catan est moins complexe que les √©checs (10‚Å¥¬≥ √©tats) mais plus que Tic-Tac-Toe

**R√®gle approximative** : L'encoder doit avoir entre 1√ó et 5√ó la taille de l'observation.

Pour Catan :
- Observation : 275 floats
- Premi√®re couche : 512 neurones (~2√ó l'observation)
- Deuxi√®me couche : 256 neurones (r√©duction progressive)
- Troisi√®me couche : 128 neurones (encodage latent compact)

‚Üí **Total : ~150k param√®tres** (raisonnable pour commencer)

---

### 4.3 Profondeur (nombre de couches)

| Profondeur | Capacit√© | Quand l'utiliser |
|------------|----------|------------------|
| **1-2 couches** | Faible | Probl√®mes lin√©aires simples |
| **3-5 couches** | **Sweet spot pour jeux de plateau** | Catan, √©checs, Go (sans CNN) |
| **6-10 couches** | Haute | Vision par ordinateur (CNN), NLP |
| **10+ couches** | Tr√®s haute | Transformers, GPT, etc. |

Pour CatanBot, **3 couches** (512 ‚Üí 256 ‚Üí 128) est un bon point de d√©part.

---

### 4.4 Fonctions d'activation

| Activation | Formule | Usage | Avantages | Inconv√©nients |
|------------|---------|-------|-----------|---------------|
| **ReLU** | max(0, x) | Couches cach√©es | Rapide, simple | Neurones "morts" (gradient=0) |
| **LeakyReLU** | max(0.01x, x) | Couches cach√©es | √âvite neurones morts | L√©g√®rement plus lent |
| **Tanh** | (e^x - e^-x) / (e^x + e^-x) | T√™te valeur | Sortie ‚àà [-1, 1] | Peut saturer |
| **Softmax** | e^xi / Œ£e^xj | T√™te politique | Sortie = probabilit√©s | N√©cessite masquage |

**Choix pour CatanBot** :
- **ReLU** dans l'encoder (standard, rapide)
- **Tanh** pour la t√™te valeur (sortie entre -1 et +1)
- **Softmax masqu√©** pour la t√™te politique (probabilit√©s sur actions l√©gales)

---

### 4.5 Normalisation

Les r√©seaux profonds (>3 couches) n√©cessitent une **normalisation** pour stabiliser l'entra√Ænement :

| Technique | Principe | Avantages | Inconv√©nients |
|-----------|----------|-----------|---------------|
| **BatchNorm** | Normalise par batch | Tr√®s efficace en vision | Comportement diff√©rent train/test |
| **LayerNorm** | Normalise par couche | Stable, simple | L√©g√®rement moins efficace |
| **GroupNorm** | Normalise par groupe | Ind√©pendant de la taille du batch | Plus complexe |

**Choix pour CatanBot** : **LayerNorm** (simple, stable, standard en RL)

---

### 4.6 M√©thode empirique pour ajuster

1. **Commence petit** : Encoder 256 ‚Üí 128 (baseline rapide)
2. **Entra√Æne 10k parties** et observe les courbes de loss
3. **Analyse les sympt√¥mes** :
   - **Loss stagne vite** (< 500 it√©rations) ‚Üí R√©seau trop petit, augmente √† 512 ‚Üí 256 ‚Üí 128
   - **Loss descend lentement mais r√©guli√®rement** ‚Üí Bon dimensionnement, continue
   - **Loss oscille beaucoup** ‚Üí Learning rate trop √©lev√© ou besoin de dropout
   - **Overfitting** (loss train baisse, loss eval augmente) ‚Üí Ajoute dropout ou r√©gularisation
4. **√âvalue sur baselines** :
   - Si l'agent bat **RandomLegal** (>90%) mais plafonne face √† **Heuristic** (<60%) :
     - Option A : Augmenter la capacit√© (r√©seau plus gros)
     - Option B : Am√©liorer l'architecture (passer √† CNN/GNN)
     - Option C : Am√©liorer l'encodage (ajouter des features)
5. **It√®re** : Le RL est un processus exp√©rimental !

---

## 5. MLP vs CNN vs GNN

**Question** : Tu as parl√© de MLP, CNN, GNN... qu'est-ce que c'est ?

**R√©ponse** : Ce sont diff√©rents types d'**encoders** (la partie qui transforme l'observation brute en repr√©sentation compacte).

---

### 5.1 MLP (Multi-Layer Perceptron) = R√©seau fully-connected

**Principe** :
```
[Input : 275 floats] ‚Üí [FC 512] ‚Üí [FC 256] ‚Üí [FC 128] ‚Üí [Output]
```
Chaque neurone est connect√© √† **tous** les neurones de la couche suivante.

**Avantages** :
- ‚úÖ **Simple** √† impl√©menter (quelques lignes PyTorch)
- ‚úÖ Fonctionne bien pour des inputs "plats" (vecteurs 1D)
- ‚úÖ Rapide √† entra√Æner

**Inconv√©nients** :
- ‚ùå Ignore la structure spatiale (ex: voisinage des hexagones)
- ‚ùå Beaucoup de param√®tres si l'input est grand

**Quand l'utiliser** : MVP, prototypage rapide, inputs sans structure spatiale

---

### 5.2 CNN (Convolutional Neural Network) = R√©seau convolutif

**Principe** :
```
[Image/Grid 2D] ‚Üí [Conv2D 3√ó3] ‚Üí [Conv2D 3√ó3] ‚Üí [Flatten] ‚Üí [FC] ‚Üí [Output]
```
Chaque neurone ne regarde qu'une **petite r√©gion** (ex: 3√ó3 pixels).

**Avantages** :
- ‚úÖ Capture la structure **spatiale** (ex: "une colonie voisine d'un hex 6 est forte")
- ‚úÖ Invariance par translation (apprend des patterns locaux r√©utilisables)
- ‚úÖ Moins de param√®tres qu'un MLP pour des images

**Inconv√©nients** :
- ‚ùå N√©cessite un encodage "image-like" (grille 2D r√©guli√®re)
- ‚ùå Catan a un plateau **hexagonal**, pas rectangulaire ‚Üí besoin de padding/interpolation

**Quand l'utiliser** : Jeux sur grille (Go, √©checs), vision par ordinateur

**Exemples c√©l√®bres** :
- AlphaGo (19√ó19 board)
- Atari (frames de jeu 84√ó84 pixels)

---

### 5.3 GNN (Graph Neural Network) = R√©seau sur graphe

**Principe** :
```
[Graphe : vertices + edges] ‚Üí [GNN layers] ‚Üí [Pooling] ‚Üí [FC] ‚Üí [Output]
```
Chaque neurone "√©change des messages" avec ses **voisins** dans le graphe.

**Avantages** :
- ‚úÖ Capture **parfaitement** la structure hexagonale de Catan (vertices, edges, tiles)
- ‚úÖ Invariance par rotation/sym√©trie (apprend des patterns g√©n√©riques)
- ‚úÖ √âl√©gant conceptuellement (le plateau EST un graphe)

**Inconv√©nients** :
- ‚ùå Plus **complexe** √† impl√©menter (n√©cessite PyTorch Geometric)
- ‚ùå Moins de ressources/tutoriels que MLP/CNN
- ‚ùå Peut √™tre plus lent √† entra√Æner

**Quand l'utiliser** : Quand le domaine a une structure de graphe naturelle

**Exemples c√©l√®bres** :
- Mol√©cules (atomes = n≈ìuds, liaisons = ar√™tes)
- R√©seaux sociaux
- Syst√®mes de recommandation

---

### 5.4 Tableau comparatif pour Catan

| Encoder | Impl√©mentation | Performance attendue | Temps dev | Justesse architecturale |
|---------|----------------|----------------------|-----------|-------------------------|
| **MLP** | Triviale | Bonne pour MVP | 1h | ‚≠ê‚≠ê |
| **CNN** | Moyenne (padding hex) | Meilleure si bien fait | 3-5h | ‚≠ê‚≠ê‚≠ê |
| **GNN** | Complexe (PyTorch Geometric) | Optimale en th√©orie | 1-2j | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

### 5.5 Strat√©gie d'it√©ration pour CatanBot

**Phase 1 (RL-005, maintenant)** : MLP 512 ‚Üí 256 ‚Üí 128
- Objectif : MVP fonctionnel rapidement
- Validation : Bat RandomLegal (>90%), commence √† challenger Heuristic

**Phase 2 (RL-006)** : Entra√Ænement PPO avec MLP
- Objectif : Atteindre ~60-65% de winrate vs Heuristic
- Si √ßa plafonne ‚Üí passer √† Phase 3

**Phase 3 (RL-011, futur)** : Am√©liorer l'architecture
- Option A : Passer √† un **CNN** avec encodage hex ‚Üí grid 2D
- Option B : Passer √† un **GNN** (PyTorch Geometric)
- Objectif : Franchir la barre des 70-75% vs Heuristic

**Phase 4 (RL-012+, futur lointain)** : Peaufinage
- Hyperparam√®tres
- Curriculum learning
- Reward shaping
- Objectif : Battre des humains experts

---

## 6. Peut-on battre des humains experts ?

**Question** : Est-on **s√ªr** d'arriver √† un r√©sultat meilleur que n'importe quel humain ?

**R√©ponse** : **Non, absolument pas !** Mais c'est un objectif atteignable avec suffisamment de travail.

---

### 6.1 Exemples de succ√®s en RL

| Jeu | Agent RL | Niveau atteint | Ann√©e | Contexte |
|-----|----------|----------------|-------|----------|
| **√âchecs** | AlphaZero | Bat Stockfish (meilleur moteur) | 2017 | 44M parties, 5000 TPUs |
| **Go** | AlphaGo | Bat Lee Sedol (champion monde) | 2016 | 30M parties, 1920 CPUs + 280 GPUs |
| **Poker** | Pluribus | Bat 5 pros simultan√©ment | 2019 | 12 jours, 64 CPU cores |
| **StarCraft II** | AlphaStar | Niveau Grandmaster (top 0.2%) | 2019 | 200 ans de temps de jeu |
| **Dota 2** | OpenAI Five | Bat l'√©quipe OG (champions TI) | 2018 | 10 mois, 128k CPU + 256 GPUs |

**Constat** : Oui, c'est possible ! Mais √ßa demande beaucoup de ressources.

---

### 6.2 Pourquoi c'est possible ?

1. **Pas de fatigue** : L'IA peut jouer des millions de parties sans pause
2. **Z√©ro biais cognitif** : Pas d'√©motions, pas de "tilt", pas d'attachement irrationnel √† une strat√©gie
3. **Exploration exhaustive** : L'IA teste des strat√©gies que les humains n'essaient jamais (ex: sacrifices contre-intuitifs)
4. **Apprentissage continu** : L'IA s'am√©liore √† chaque partie, les humains ont un plateau

---

### 6.3 Pourquoi √ßa pourrait √©chouer pour CatanBot ?

1. **Pas assez de donn√©es** : AlphaZero a jou√© 44M parties, on vise 1-10M pour CatanBot
2. **Architecture inadapt√©e** : Si le MLP ne capture pas la complexit√©, et qu'on ne passe pas √† CNN/GNN
3. **Reward mal d√©fini** :
   - Si on r√©compense seulement victoire/d√©faite (+1/-1), l'agent peut stagner
   - Si on r√©compense trop de sous-objectifs (reward shaping), l'agent peut "tricher"
4. **Hasard** : Catan a beaucoup de randomness (d√©s, cartes dev) ‚Üí plus dur d'apprendre qu'aux √©checs
5. **Ressources limit√©es** : On n'a pas 5000 TPUs comme DeepMind üòÖ

---

### 6.4 Solutions si l'agent plafonne

#### Solution 1 : Augmenter la capacit√© du r√©seau
- Passer de MLP (512 ‚Üí 256 ‚Üí 128) √† MLP plus gros (1024 ‚Üí 512 ‚Üí 256)
- Passer √† CNN ou GNN
- Ajouter plus de couches

**Quand l'utiliser** : Si la loss descend r√©guli√®rement mais l'agent reste faible vs Heuristic

---

#### Solution 2 : Am√©liorer l'encodage
- Ajouter des features calcul√©es (ex: "distance √† la prochaine colonie l√©gale", "probabilit√© d'obtenir X ressources dans 3 tours")
- Encoder l'historique (ex: "combien de fois j'ai jou√© un chevalier dans les 5 derniers tours")
- Normalisation adaptative

**Quand l'utiliser** : Si l'agent fait des erreurs "stupides" (ex: ne voit pas une opportunit√© √©vidente)

---

#### Solution 3 : Curriculum learning
- Faire jouer l'agent contre des adversaires de plus en plus forts :
  1. RandomLegal (niveau 0)
  2. Heuristique basique (niveau 1)
  3. Heuristique avanc√©e (niveau 2)
  4. Self-play (niveau 3)
- Augmenter progressivement la difficult√©

**Quand l'utiliser** : Si l'agent stagne en self-play (pas de signal d'apprentissage)

---

#### Solution 4 : Reward shaping
- R√©compenser les sous-objectifs :
  - +0.1 pour chaque colonie construite
  - +0.2 pour chaque ville construite
  - +0.05 pour obtenir le chemin le plus long
  - -0.05 pour se faire voler par le voleur
- **Danger** : Si les r√©compenses interm√©diaires sont mal calibr√©es, l'agent peut tricher (optimiser les sous-objectifs sans gagner)

**Quand l'utiliser** : En dernier recours, avec pr√©caution

---

#### Solution 5 : Jouer plus de parties
- AlphaZero a jou√© **44 millions** de parties d'√©checs
- On peut viser :
  - **Phase 1** : 100k parties (validation MVP)
  - **Phase 2** : 1M parties (challenger Heuristic)
  - **Phase 3** : 10M parties (battre des humains bons)

**Quand l'utiliser** : Toujours ! Plus de donn√©es = meilleur agent (jusqu'√† un plateau)

---

#### Solution 6 : Ensembling
- Entra√Æner **plusieurs agents** avec des seeds/hyperparam√®tres diff√©rents
- √Ä l'√©valuation, faire voter les agents ou moyenner leurs politiques
- Technique utilis√©e par AlphaGo

**Quand l'utiliser** : Quand on veut maximiser la robustesse pour une comp√©tition

---

### 6.5 Objectif r√©aliste pour CatanBot

| Phase | Objectif | Ressources | √âch√©ance |
|-------|----------|------------|----------|
| **MVP (RL-005 √† RL-007)** | Bat RandomLegal (>90%) | 100k parties, 1 GPU, 1 semaine | Court terme |
| **V1 (RL-008 √† RL-010)** | Bat Heuristic (>65%) | 1M parties, 1 GPU, 1 mois | Moyen terme |
| **V2 (futur)** | Bat humains bons (>70%) | 5-10M parties, 1-4 GPUs, 3-6 mois | Long terme |
| **V3 (futur lointain)** | Bat humains experts (>80%) | 20M+ parties, architecture avanc√©e | Tr√®s long terme |

---

## 7. Nos choix pour CatanBot

### 7.1 R√©capitulatif des d√©cisions techniques

| D√©cision | Justification |
|----------|---------------|
| **Architecture 2 t√™tes partag√©es** | Efficace, prouv√© (AlphaZero, PPO), standard pour jeux de plateau |
| **Encoder MLP** | Simple pour MVP, on pourra it√©rer vers CNN/GNN apr√®s √©valuation |
| **Taille 512 ‚Üí 256 ‚Üí 128** | Compromis capacit√©/vitesse pour Catan (~150k param√®tres) |
| **ReLU + LayerNorm** | Standard, stable, rapide |
| **Masquage d'actions** | Indispensable pour √©viter les actions ill√©gales |
| **Perspective ego-centr√©e** | D√©j√† impl√©ment√©e dans ObservationTensor (RL-001) |

---

### 7.2 Architecture d√©taill√©e pour RL-005

```python
class CatanPolicyValueNetwork(nn.Module):
    def __init__(self, obs_size: int, action_size: int, hidden_sizes: List[int] = [512, 256, 128]):
        # Encoder (MLP)
        self.encoder = nn.Sequential(
            nn.Linear(obs_size, hidden_sizes[0]),
            nn.ReLU(),
            nn.LayerNorm(hidden_sizes[0]),
            nn.Linear(hidden_sizes[0], hidden_sizes[1]),
            nn.ReLU(),
            nn.LayerNorm(hidden_sizes[1]),
            nn.Linear(hidden_sizes[1], hidden_sizes[2]),
            nn.ReLU(),
            nn.LayerNorm(hidden_sizes[2]),
        )

        # T√™te politique
        self.policy_head = nn.Linear(hidden_sizes[2], action_size)

        # T√™te valeur
        self.value_head = nn.Sequential(
            nn.Linear(hidden_sizes[2], 1),
            nn.Tanh()  # Sortie ‚àà [-1, 1]
        )

    def forward(self, obs: torch.Tensor, mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # Encoder
        features = self.encoder(obs)  # (batch, 128)

        # Politique
        logits = self.policy_head(features)  # (batch, action_size)
        masked_logits = torch.where(mask, logits, torch.tensor(-float('inf')))

        # Valeur
        value = self.value_head(features)  # (batch, 1)

        return masked_logits, value
```

---

### 7.3 Tests de validation (RL-005)

Les tests unitaires v√©rifieront :

1. **Shapes** :
   - Input : (batch, obs_size) + (batch, action_size) mask
   - Output : (batch, action_size) logits + (batch, 1) value

2. **Masquage** :
   - Les logits masqu√©s contiennent `-inf` aux bons endroits
   - Softmax(masked_logits) donne 0% aux actions ill√©gales

3. **Valeur** :
   - La sortie de la t√™te valeur est bien ‚àà [-1, 1] (gr√¢ce √† Tanh)

4. **Gradient flow** :
   - Backpropagation fonctionne (pas de gradient=0 partout)

5. **Int√©gration** :
   - Le r√©seau accepte un vrai `ObservationTensor` (de RL-001)
   - Le masque vient d'un vrai `ActionEncoder` (de RL-002)

---

### 7.4 Prochaines √©tapes apr√®s RL-005

1. **RL-006** : Entra√Ænement PPO avec masques
   - Impl√©menter la boucle d'entra√Ænement (collect experiences ‚Üí update network)
   - Valider que la loss descend

2. **RL-007** : √âvaluation p√©riodique vs baselines
   - Tous les N checkpoints, lancer 200 parties vs RandomLegal et Heuristic
   - Logger winrate, ELO, etc.

3. **RL-008** : Auto-play miroir et ligue
   - Self-play : l'agent joue contre lui-m√™me (ou des versions pass√©es)
   - Cr√©er une "ligue" d'adversaires

4. **RL-011 (futur)** : Am√©liorer l'architecture
   - Si le MLP plafonne, tester CNN ou GNN

---

## Conclusion

Le Reinforcement Learning est un domaine **empirique** : on construit des hypoth√®ses, on teste, on ajuste. Il n'y a pas de formule magique pour garantir qu'un agent battra des humains experts, mais les succ√®s d'AlphaZero, AlphaGo et autres montrent que c'est possible avec :

1. Une **architecture adapt√©e** (on commence avec 2 t√™tes + MLP)
2. Un **encodage pertinent** (perspective ego-centr√©e, normalisation)
3. Du **volume** (jouer des millions de parties)
4. De l'**it√©ration** (ajuster l'architecture, les hyperparam√®tres, le curriculum)

CatanBot est sur la bonne voie ! üöÄ

---

## Ressources compl√©mentaires

- **Cours** :
  - [Spinning Up in Deep RL (OpenAI)](https://spinningup.openai.com/)
  - [DeepMind x UCL Deep RL Course](https://www.deepmind.com/learning-resources/deep-reinforcement-learning-lecture-series-2021)

- **Papers** :
  - AlphaZero : "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm" (2017)
  - PPO : "Proximal Policy Optimization Algorithms" (2017)

- **Code** :
  - [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3) : Impl√©mentations RL en PyTorch
  - [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/) : Pour les GNN (futur)

---

**Document r√©dig√© pour CatanBot - 2025**
**Auteur : Claude (Agent RL) + Thomas (Human Coach)** ü§ñü§ùüë®‚Äçüíª
